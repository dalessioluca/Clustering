{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST VAE with enumeration of discrete latent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as tvt\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO #, TraceGraph_ELBO\n",
    "from pyro.optim import Adam, Adamax, Adadelta, SGD\n",
    "from torchvision import utils\n",
    "from matplotlib.pyplot import imshow\n",
    "from torch.distributions import constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(images,nrow=4,npadding=10):\n",
    "    \"\"\"Visualize a torch tensor of shape: (batch x ch x width x height) \"\"\"\n",
    "    batch, ch, width, height = images.shape\n",
    "    if(images.device != \"cpu\"):\n",
    "        images=images.cpu()\n",
    "    grid = utils.make_grid(images,nrow, npadding, normalize=True, range=None, scale_each=True, pad_value=1)       \n",
    "    imshow(grid.detach().numpy().transpose((1, 2, 0))) \n",
    "    \n",
    "def train_one_epoch(svi, dataloader, verbose=False):\n",
    "    epoch_loss = 0.\n",
    "    n = 0\n",
    "    for i, data in enumerate(dataloader, 0): #loop over minibatches\n",
    "            \n",
    "        # Get images and flatten them into vectors\n",
    "        imgs,labels= data\n",
    "        n += len(labels)\n",
    "        loss = svi.step(imgs)\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"i= %3d train_loss=%.5f\" %(i,loss))\n",
    "        epoch_loss += loss\n",
    "\n",
    "    return epoch_loss / n\n",
    "\n",
    "def evaluate_one_epoch(svi, dataloader, verbose=False):\n",
    "    epoch_loss = 0.\n",
    "    n = 0\n",
    "    for i, data in enumerate(dataloader, 0): #loop over minibatches\n",
    "            \n",
    "        # Get images and flatten them into vectors\n",
    "        imgs,labels= data\n",
    "        n += len(labels)\n",
    "        loss = svi.evaluate_loss(imgs)\n",
    "        \n",
    "        if(verbose):\n",
    "            print(\"i= %3d  test_loss=%.5f\" %(i,loss))\n",
    "        epoch_loss += loss\n",
    "\n",
    "    return epoch_loss / n\n",
    "\n",
    "def save_obj(obj,root_dir,name):\n",
    "    with open(root_dir + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(root_dir,name):\n",
    "    with open(root_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_model(model, root_dir, name):\n",
    "    full_file_path= root_dir + name + '.pkl'\n",
    "    torch.save(model.state_dict(),full_file_path)\n",
    "    \n",
    "def load_model(model, root_dir, name):\n",
    "    full_file_path= root_dir + name + '.pkl'\n",
    "    model.load_state_dict(torch.load(full_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 10, 'CHANNELS': 1, 'WIDTH': 28, 'HEIGHT': 28, 'Z_DIM': 2, 'use_cuda': False}\n"
     ]
    }
   ],
   "source": [
    "params = { \n",
    "    'K' : 10, #number of clusters\n",
    "    'CHANNELS' : 1,\n",
    "    'WIDTH' : 28,\n",
    "    'HEIGHT':28,\n",
    "    'Z_DIM':2,\n",
    "    'use_cuda' : torch.cuda.is_available()\n",
    "    }\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "kwargs = {'num_workers': 1, 'pin_memory': params[\"use_cuda\"]}\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./data', train=True, download=True, transform=tvt.ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "testset = torchvision.datasets.MNIST('./data', train=False, download=True, transform=tvt.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "\n",
    "imgs,labels = next(iter(testloader))\n",
    "\n",
    "print(imgs.shape)\n",
    "show_batch(imgs[:8],npadding=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the VAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENCODER(torch.nn.Module):\n",
    "    \"\"\" x -> p \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.K      = params['K']\n",
    "        self.ch     = params['CHANNELS']\n",
    "        self.width  = params['WIDTH']\n",
    "        self.height = params['HEIGHT']\n",
    "        self.z_dim  = params['Z_DIM']\n",
    "        self.x_dim = self.ch*self.width*self.height\n",
    "        self.comp_p  = torch.nn.Linear(self.x_dim,self.K, bias=True)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = x.view(batch_size,-1)\n",
    "        p = self.softmax(self.comp_p(x1))\n",
    "        return p\n",
    "    \n",
    "class DECODER(torch.nn.Module):\n",
    "    \"\"\" z -> x \"\"\"\n",
    "    \n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.ch     = params['CHANNELS']\n",
    "        self.width  = params['WIDTH']\n",
    "        self.height = params['HEIGHT']\n",
    "        self.z_dim  = params['Z_DIM']\n",
    "        self.x_dim = self.ch*self.width*self.height\n",
    "        \n",
    "        self.comp_x_mu  = torch.nn.Linear(self.z_dim,self.x_dim, bias=True)\n",
    "         \n",
    "    def forward(self,z):\n",
    "        batch_size = z.shape[0]\n",
    "        x_mu = torch.sigmoid(self.comp_x_mu(z)).view(batch_size,self.ch,self.height,self.width)\n",
    "        return x_mu\n",
    "        \n",
    "\n",
    "\n",
    "class VAE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,params,encoder,decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.use_cuda = params['use_cuda']\n",
    "        self.K      = params['K']\n",
    "        self.ch     = params['CHANNELS']\n",
    "        self.width  = params['WIDTH']\n",
    "        self.height = params['HEIGHT']\n",
    "        self.z_dim  = params['Z_DIM']\n",
    "        self.x_dim = self.ch*self.width*self.height\n",
    "        \n",
    "        # Instantiate the encoder and decoder\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        if(self.use_cuda):\n",
    "            self.cuda()\n",
    "        \n",
    "    def guide(self, imgs=None):\n",
    "        \"\"\" 1. run the inference to get: zwhere,zwhat\n",
    "            2. sample latent variables \n",
    "        \"\"\"       \n",
    "        #-----------------------#\n",
    "        #--------  Trick -------#\n",
    "        #-----------------------#\n",
    "        if(imgs is None):\n",
    "            observed = False\n",
    "            imgs = torch.zeros(8,self.ch,self.height,self.width)\n",
    "            if(self.use_cuda):\n",
    "                imgs=imgs.cuda()\n",
    "        else:\n",
    "            observed = True\n",
    "        #-----------------------#\n",
    "        #----- Enf of Trick ----#\n",
    "        #-----------------------#\n",
    "\n",
    "        batch_size,ch,width,height = imgs.shape\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "\n",
    "        # Global variables\n",
    "        locs_mu = pyro.param(\"locs_mu\",imgs.new_zeros(self.K,self.z_dim))\n",
    "        locs_q = pyro.sample('locs', dist.Delta(locs_mu).to_event(1))\n",
    "        \n",
    "        scales_mu = pyro.param(\"scales_mu\",imgs.new_ones(self.K,self.z_dim),constraint=constraints.positive)\n",
    "        scales_q = pyro.sample('scales', dist.Delta(scales_mu).to_event(1))\n",
    "        \n",
    "        concentrations = pyro.param(\"concentrations\",imgs.new_ones(self.K),constraint=constraints.positive)\n",
    "        weights_q = pyro.sample('weights', dist.Dirichlet(concentrations))\n",
    "        \n",
    "        with pyro.plate('batch_size', batch_size, dim=-1):\n",
    "            p = self.encoder(imgs)\n",
    "            assignment = pyro.sample('assignment',dist.Categorical(p))\n",
    "            z = pyro.sample('z_style', dist.Normal(locs_q[assignment], scales_q[assignment]).to_event(1))\n",
    "        return p,z\n",
    "            \n",
    "    def model(self, imgs=None):\n",
    "        \"\"\" 1. sample the latent from the prior:\n",
    "                - z_type ~ Categorical(weights) \n",
    "                - z_style ~ N(loc,scale) \n",
    "            2. runs the generative model\n",
    "            3. score the generative model against actual data \n",
    "        \"\"\"\n",
    "        #-----------------------#\n",
    "        #--------  Trick -------#\n",
    "        #-----------------------#\n",
    "        if(imgs is None):\n",
    "            observed = False\n",
    "            imgs = torch.zeros(8,self.ch,self.height,self.width)\n",
    "            if(self.use_cuda):\n",
    "                imgs=imgs.cuda()\n",
    "        else:\n",
    "            observed = True\n",
    "        #-----------------------#\n",
    "        #----- Enf of Trick ----#\n",
    "        #-----------------------#\n",
    "            \n",
    "        batch_size,ch,width,height = imgs.shape\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        \n",
    "        # Global variable\n",
    "        weights = pyro.sample('weights', dist.Dirichlet(0.5 * imgs.new_ones(self.K)))\n",
    "\n",
    "        # Variable for each components\n",
    "        with pyro.plate('components', self.K, dim=-1):\n",
    "            locs = pyro.sample('locs', dist.Normal(0., 10.*imgs.new_ones(self.K,self.z_dim)).to_event(1))\n",
    "            scales = pyro.sample('scales', dist.LogNormal(0., 2.*imgs.new_ones(self.K,self.z_dim)).to_event(1))\n",
    "\n",
    "\n",
    "        # Variable for each image\n",
    "        with pyro.plate('batch_size', batch_size, dim=-1):\n",
    "            assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "            z = pyro.sample('z_style', dist.Normal(locs[assignment], scales[assignment]).to_event(1))\n",
    "            x_mu = self.decoder(z) #x_mu is between 0 and 1\n",
    "            pyro.sample('obs', dist.Bernoulli(x_mu.view(-1,self.x_dim)).to_event(1), obs=imgs.view(-1,self.x_dim))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ENCODER(params)\n",
    "decoder = DECODER(params)\n",
    "vae = VAE(params,encoder,decoder)\n",
    "vae.guide()\n",
    "vae.model()\n",
    "\n",
    "load_model(vae,\"/Users/ldalessi/ENUMERATION_MNIST/ARCHIVE/\",\"vae_v5_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,labels = next(iter(testloader))\n",
    "\n",
    "p,z = vae.guide(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])\n",
      "tensor([[0.1074, 0.1003, 0.1025, 0.1236, 0.0977, 0.1023, 0.1028, 0.0917, 0.0835,\n",
      "         0.0882],\n",
      "        [0.0833, 0.0993, 0.0934, 0.0949, 0.0809, 0.1552, 0.0972, 0.1040, 0.1048,\n",
      "         0.0869],\n",
      "        [0.1117, 0.1130, 0.0855, 0.1154, 0.0909, 0.1170, 0.0910, 0.0954, 0.0860,\n",
      "         0.0941],\n",
      "        [0.0858, 0.1033, 0.1074, 0.0855, 0.0968, 0.1170, 0.1566, 0.0799, 0.0904,\n",
      "         0.0775],\n",
      "        [0.1101, 0.1047, 0.0973, 0.1113, 0.0931, 0.1060, 0.0966, 0.0997, 0.1019,\n",
      "         0.0793],\n",
      "        [0.1127, 0.1082, 0.0858, 0.1171, 0.0899, 0.1270, 0.0927, 0.0974, 0.0780,\n",
      "         0.0911],\n",
      "        [0.1170, 0.0985, 0.1106, 0.1157, 0.0874, 0.1222, 0.0994, 0.0963, 0.0750,\n",
      "         0.0778],\n",
      "        [0.0907, 0.1053, 0.1227, 0.1080, 0.0780, 0.1150, 0.0839, 0.0933, 0.1116,\n",
      "         0.0916],\n",
      "        [0.0941, 0.1124, 0.0955, 0.0861, 0.0845, 0.1227, 0.1055, 0.1022, 0.0926,\n",
      "         0.1042],\n",
      "        [0.1283, 0.0837, 0.1105, 0.1470, 0.1002, 0.0854, 0.1085, 0.0869, 0.0688,\n",
      "         0.0807],\n",
      "        [0.0851, 0.1029, 0.0907, 0.0861, 0.1064, 0.1380, 0.1063, 0.0872, 0.1069,\n",
      "         0.0905],\n",
      "        [0.0810, 0.0989, 0.0844, 0.0881, 0.0784, 0.1339, 0.1430, 0.1009, 0.1154,\n",
      "         0.0761],\n",
      "        [0.1098, 0.0954, 0.1499, 0.1208, 0.0795, 0.0941, 0.1098, 0.0833, 0.0818,\n",
      "         0.0755],\n",
      "        [0.0927, 0.0928, 0.1052, 0.0935, 0.1238, 0.1325, 0.1221, 0.0792, 0.0860,\n",
      "         0.0722],\n",
      "        [0.1024, 0.1181, 0.1168, 0.0920, 0.0831, 0.1175, 0.1037, 0.0821, 0.0885,\n",
      "         0.0957],\n",
      "        [0.1042, 0.1092, 0.1144, 0.1027, 0.0853, 0.1321, 0.0762, 0.0765, 0.1127,\n",
      "         0.0867],\n",
      "        [0.1262, 0.0977, 0.1216, 0.1048, 0.0821, 0.1198, 0.1171, 0.0820, 0.0809,\n",
      "         0.0679],\n",
      "        [0.0944, 0.0961, 0.1091, 0.1005, 0.0920, 0.0959, 0.1212, 0.1038, 0.0953,\n",
      "         0.0916],\n",
      "        [0.0901, 0.0927, 0.1213, 0.1253, 0.0688, 0.1136, 0.1042, 0.0966, 0.1029,\n",
      "         0.0845],\n",
      "        [0.1152, 0.0915, 0.1175, 0.1297, 0.0895, 0.1019, 0.1046, 0.0877, 0.0872,\n",
      "         0.0755],\n",
      "        [0.1335, 0.0897, 0.1025, 0.1549, 0.0807, 0.0993, 0.1043, 0.0799, 0.0717,\n",
      "         0.0836],\n",
      "        [0.1150, 0.1027, 0.1073, 0.0892, 0.0748, 0.0976, 0.1057, 0.1077, 0.1091,\n",
      "         0.0910],\n",
      "        [0.1080, 0.1112, 0.0891, 0.0969, 0.0631, 0.1213, 0.1093, 0.1021, 0.1132,\n",
      "         0.0859],\n",
      "        [0.1022, 0.1225, 0.1299, 0.0712, 0.1107, 0.1056, 0.0909, 0.0819, 0.0911,\n",
      "         0.0940],\n",
      "        [0.1071, 0.1013, 0.0993, 0.1094, 0.0918, 0.1031, 0.0914, 0.0988, 0.1146,\n",
      "         0.0833],\n",
      "        [0.0966, 0.0995, 0.0890, 0.0947, 0.0988, 0.1258, 0.1341, 0.0715, 0.0745,\n",
      "         0.1156],\n",
      "        [0.0934, 0.0947, 0.1006, 0.0952, 0.0972, 0.0944, 0.0957, 0.1178, 0.1148,\n",
      "         0.0963],\n",
      "        [0.1038, 0.0841, 0.1177, 0.1132, 0.0974, 0.1020, 0.1092, 0.0923, 0.0966,\n",
      "         0.0836],\n",
      "        [0.1027, 0.0896, 0.1032, 0.0824, 0.1179, 0.1181, 0.1220, 0.1053, 0.0823,\n",
      "         0.0764],\n",
      "        [0.1051, 0.1042, 0.1058, 0.1031, 0.0914, 0.1097, 0.0954, 0.1025, 0.0964,\n",
      "         0.0865],\n",
      "        [0.1231, 0.1206, 0.1122, 0.0923, 0.0840, 0.1090, 0.1143, 0.0842, 0.0871,\n",
      "         0.0733],\n",
      "        [0.1044, 0.1133, 0.0997, 0.0996, 0.0909, 0.1182, 0.0889, 0.0941, 0.0977,\n",
      "         0.0931],\n",
      "        [0.1057, 0.1021, 0.1087, 0.1208, 0.0957, 0.1462, 0.0867, 0.0830, 0.0841,\n",
      "         0.0670],\n",
      "        [0.0859, 0.1153, 0.0800, 0.1071, 0.0838, 0.1159, 0.1298, 0.0676, 0.0965,\n",
      "         0.1182],\n",
      "        [0.1429, 0.0944, 0.0896, 0.1126, 0.1138, 0.1006, 0.1202, 0.0842, 0.0787,\n",
      "         0.0630],\n",
      "        [0.1155, 0.1040, 0.0730, 0.0797, 0.1361, 0.1210, 0.0728, 0.0871, 0.1062,\n",
      "         0.1046],\n",
      "        [0.1136, 0.1277, 0.0971, 0.0968, 0.0955, 0.1012, 0.1132, 0.0996, 0.0834,\n",
      "         0.0719],\n",
      "        [0.1007, 0.1112, 0.1156, 0.0913, 0.0924, 0.1046, 0.0972, 0.0959, 0.0925,\n",
      "         0.0987],\n",
      "        [0.0900, 0.1258, 0.1189, 0.0792, 0.0892, 0.1006, 0.1111, 0.0873, 0.1071,\n",
      "         0.0909],\n",
      "        [0.1050, 0.1066, 0.1238, 0.1013, 0.0797, 0.1265, 0.0968, 0.0877, 0.0856,\n",
      "         0.0871],\n",
      "        [0.0977, 0.1045, 0.1024, 0.0974, 0.0940, 0.1065, 0.0991, 0.0931, 0.0932,\n",
      "         0.1121],\n",
      "        [0.1063, 0.1330, 0.0963, 0.0879, 0.0885, 0.1179, 0.1009, 0.0857, 0.0959,\n",
      "         0.0877],\n",
      "        [0.1100, 0.1051, 0.1295, 0.1184, 0.0800, 0.1134, 0.0964, 0.0876, 0.0863,\n",
      "         0.0734],\n",
      "        [0.0878, 0.1380, 0.0998, 0.1053, 0.0749, 0.1272, 0.0766, 0.0920, 0.0978,\n",
      "         0.1006],\n",
      "        [0.0949, 0.1174, 0.0994, 0.0969, 0.1045, 0.1279, 0.0912, 0.0881, 0.1012,\n",
      "         0.0786],\n",
      "        [0.0987, 0.1097, 0.1042, 0.0771, 0.0972, 0.1090, 0.1041, 0.1067, 0.0851,\n",
      "         0.1083],\n",
      "        [0.0978, 0.1071, 0.0975, 0.0927, 0.1115, 0.1296, 0.0866, 0.0889, 0.1074,\n",
      "         0.0807],\n",
      "        [0.0974, 0.1180, 0.0838, 0.1004, 0.0882, 0.1391, 0.0981, 0.1023, 0.0842,\n",
      "         0.0886],\n",
      "        [0.1118, 0.0898, 0.1388, 0.1103, 0.0904, 0.1263, 0.1009, 0.0831, 0.0839,\n",
      "         0.0647],\n",
      "        [0.1066, 0.0918, 0.1084, 0.1212, 0.0941, 0.1076, 0.1056, 0.1181, 0.0744,\n",
      "         0.0723],\n",
      "        [0.1045, 0.1100, 0.1032, 0.1003, 0.0902, 0.0996, 0.1000, 0.1181, 0.0822,\n",
      "         0.0919],\n",
      "        [0.0949, 0.0921, 0.1207, 0.1198, 0.0840, 0.1158, 0.1285, 0.0880, 0.0828,\n",
      "         0.0735],\n",
      "        [0.0907, 0.0935, 0.1469, 0.1012, 0.0906, 0.0929, 0.1152, 0.0968, 0.0857,\n",
      "         0.0866],\n",
      "        [0.1025, 0.1004, 0.1019, 0.1148, 0.0986, 0.1345, 0.0824, 0.0876, 0.0792,\n",
      "         0.0982],\n",
      "        [0.0980, 0.1084, 0.0914, 0.1118, 0.0883, 0.1323, 0.1201, 0.0822, 0.0822,\n",
      "         0.0854],\n",
      "        [0.1126, 0.0925, 0.1212, 0.0797, 0.1127, 0.1051, 0.1151, 0.1068, 0.0736,\n",
      "         0.0807],\n",
      "        [0.1122, 0.0951, 0.1064, 0.0986, 0.0997, 0.1222, 0.1019, 0.1022, 0.0946,\n",
      "         0.0669],\n",
      "        [0.1000, 0.1114, 0.0894, 0.1199, 0.0907, 0.1204, 0.0983, 0.0927, 0.0833,\n",
      "         0.0939],\n",
      "        [0.1052, 0.0723, 0.1553, 0.1130, 0.0910, 0.0958, 0.1166, 0.1069, 0.0774,\n",
      "         0.0665],\n",
      "        [0.1201, 0.1015, 0.1007, 0.1172, 0.0854, 0.0907, 0.0980, 0.0997, 0.0888,\n",
      "         0.0981],\n",
      "        [0.1056, 0.0824, 0.1055, 0.0941, 0.0987, 0.0864, 0.1210, 0.1208, 0.0967,\n",
      "         0.0887],\n",
      "        [0.0989, 0.1303, 0.1145, 0.0799, 0.1058, 0.1092, 0.1153, 0.0631, 0.0873,\n",
      "         0.0955],\n",
      "        [0.1166, 0.1169, 0.0992, 0.1037, 0.0909, 0.1107, 0.1024, 0.0865, 0.0875,\n",
      "         0.0856],\n",
      "        [0.1244, 0.1110, 0.1046, 0.1059, 0.1027, 0.1318, 0.0815, 0.0810, 0.0840,\n",
      "         0.0731]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----MODEL---------\n",
      "             Trace Shapes:            \n",
      "              Param Sites:            \n",
      "decoder$$$comp_x_mu.weight 784   2    \n",
      "  decoder$$$comp_x_mu.bias     784    \n",
      "             Sample Sites:            \n",
      "              weights dist       |  10\n",
      "                     value       |  10\n",
      "                  log_prob       |    \n",
      "           components dist       |    \n",
      "                     value  10   |    \n",
      "                  log_prob       |    \n",
      "                 locs dist  10   |   2\n",
      "                     value  10   |   2\n",
      "                  log_prob  10   |    \n",
      "               scales dist  10   |   2\n",
      "                     value  10   |   2\n",
      "                  log_prob  10   |    \n",
      "           batch_size dist       |    \n",
      "                     value   8   |    \n",
      "                  log_prob       |    \n",
      "           assignment dist   8   |    \n",
      "                     value   8   |    \n",
      "                  log_prob   8   |    \n",
      "              z_style dist   8   |   2\n",
      "                     value   8   |   2\n",
      "                  log_prob   8   |    \n",
      "                  obs dist   8   | 784\n",
      "                     value   8   | 784\n",
      "                  log_prob   8   |    \n",
      "----GUIDE ---------\n",
      "          Trace Shapes:          \n",
      "           Param Sites:          \n",
      "encoder$$$comp_p.weight 10 784   \n",
      "  encoder$$$comp_p.bias     10   \n",
      "                locs_mu 10   2   \n",
      "              scales_mu 10   2   \n",
      "         concentrations     10   \n",
      "          Sample Sites:          \n",
      "              locs dist 10   |  2\n",
      "                  value 10   |  2\n",
      "               log_prob 10   |   \n",
      "            scales dist 10   |  2\n",
      "                  value 10   |  2\n",
      "               log_prob 10   |   \n",
      "           weights dist      | 10\n",
      "                  value      | 10\n",
      "               log_prob      |   \n",
      "        batch_size dist      |   \n",
      "                  value  8   |   \n",
      "               log_prob      |   \n",
      "        assignment dist  8   |   \n",
      "                  value  8   |   \n",
      "               log_prob  8   |   \n",
      "           z_style dist  8   |  2\n",
      "                  value  8   |  2\n",
      "               log_prob  8   |   \n"
     ]
    }
   ],
   "source": [
    "import pyro.poutine as poutine\n",
    "#\n",
    "print(\"----MODEL---------\")\n",
    "trace = poutine.trace(vae.model).get_trace()\n",
    "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "print(trace.format_shapes())\n",
    "#\n",
    "print(\"----GUIDE ---------\")\n",
    "trace = poutine.trace(vae.guide).get_trace()\n",
    "trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, guide, loss):\n",
    "    pyro.clear_param_store()\n",
    "    loss.loss(model, guide)\n",
    "    \n",
    "#with torch.autograd.set_detect_anomaly(True):\n",
    "test_model(vae.model,vae.guide,Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(0)\n",
    "\n",
    "TEST_FREQUENCY = 5\n",
    "WRITE_FREQUENCY = 20\n",
    "smoke_test= False\n",
    "if(smoke_test):\n",
    "    pyro.enable_validation(True)\n",
    "    pyro.distributions.enable_validation(True)\n",
    "    NUM_EPOCHS = 21\n",
    "else:\n",
    "    pyro.enable_validation(False)\n",
    "    pyro.distributions.enable_validation(False)\n",
    "    NUM_EPOCHS = 10001\n",
    "    \n",
    "    \n",
    "# setup the optimizer\n",
    "adam_args = {\"lr\": 1.0e-4}\n",
    "#optimizer = Adamax(adam_args)\n",
    "optimizer = Adadelta(adam_args)\n",
    "svi = SVI(vae.model, vae.guide, optimizer, loss=Trace_ELBO(num_particles=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=   0 train_loss=36621.85756\n",
      "i=   1 train_loss=36535.48874\n",
      "i=   2 train_loss=36608.92174\n",
      "i=   3 train_loss=36781.22895\n",
      "i=   4 train_loss=36448.91198\n",
      "i=   5 train_loss=36396.60172\n",
      "i=   6 train_loss=36641.33900\n",
      "i=   7 train_loss=36285.51387\n",
      "i=   8 train_loss=37063.77878\n",
      "i=   9 train_loss=37051.21528\n",
      "i=  10 train_loss=36287.32004\n",
      "i=  11 train_loss=36706.77610\n",
      "i=  12 train_loss=36501.66359\n",
      "i=  13 train_loss=36404.47789\n",
      "i=  14 train_loss=36596.67410\n",
      "i=  15 train_loss=36321.66892\n",
      "i=  16 train_loss=36942.99411\n",
      "i=  17 train_loss=36852.29303\n",
      "i=  18 train_loss=36619.99759\n",
      "i=  19 train_loss=36460.24059\n",
      "i=  20 train_loss=36370.08912\n",
      "i=  21 train_loss=36949.95376\n",
      "i=  22 train_loss=36603.85067\n",
      "i=  23 train_loss=36499.97131\n",
      "i=  24 train_loss=36902.52311\n",
      "i=  25 train_loss=36703.72030\n",
      "i=  26 train_loss=36757.42356\n",
      "i=  27 train_loss=36827.98165\n",
      "i=  28 train_loss=36696.44131\n",
      "i=  29 train_loss=36502.62157\n",
      "i=  30 train_loss=36610.94468\n",
      "i=  31 train_loss=36648.66165\n",
      "i=  32 train_loss=36694.49053\n",
      "i=  33 train_loss=36739.11581\n",
      "i=  34 train_loss=36745.48053\n",
      "i=  35 train_loss=36244.95929\n",
      "i=  36 train_loss=36308.38016\n",
      "i=  37 train_loss=36458.64750\n",
      "i=  38 train_loss=36943.28112\n",
      "i=  39 train_loss=36640.91194\n",
      "i=  40 train_loss=36877.83642\n",
      "i=  41 train_loss=37028.02615\n",
      "i=  42 train_loss=36768.06725\n",
      "i=  43 train_loss=36888.57287\n",
      "i=  44 train_loss=37023.62011\n",
      "i=  45 train_loss=36694.40185\n",
      "i=  46 train_loss=37172.91254\n",
      "i=  47 train_loss=36479.44184\n",
      "i=  48 train_loss=36636.40224\n",
      "i=  49 train_loss=36646.45582\n",
      "i=  50 train_loss=36681.48946\n",
      "i=  51 train_loss=37039.12220\n",
      "i=  52 train_loss=36857.27330\n",
      "i=  53 train_loss=36655.88915\n",
      "i=  54 train_loss=36237.33760\n",
      "i=  55 train_loss=36753.49828\n",
      "i=  56 train_loss=36435.32113\n",
      "i=  57 train_loss=36925.36991\n",
      "i=  58 train_loss=36676.88478\n",
      "i=  59 train_loss=37068.05904\n",
      "i=  60 train_loss=36524.96126\n",
      "i=  61 train_loss=36226.31236\n",
      "i=  62 train_loss=36885.82346\n",
      "i=  63 train_loss=36971.60392\n",
      "i=  64 train_loss=36971.30278\n",
      "i=  65 train_loss=36642.78100\n",
      "i=  66 train_loss=36357.41532\n",
      "i=  67 train_loss=36980.34402\n",
      "i=  68 train_loss=37012.83319\n",
      "i=  69 train_loss=36983.70337\n",
      "i=  70 train_loss=36566.13704\n",
      "i=  71 train_loss=36998.14741\n",
      "i=  72 train_loss=36620.07934\n",
      "i=  73 train_loss=36827.23019\n",
      "i=  74 train_loss=36741.96699\n",
      "i=  75 train_loss=36808.56686\n",
      "i=  76 train_loss=36791.43344\n",
      "i=  77 train_loss=36607.52377\n",
      "i=  78 train_loss=37105.24110\n",
      "i=  79 train_loss=36328.55579\n",
      "i=  80 train_loss=36382.26274\n",
      "i=  81 train_loss=36152.13340\n",
      "i=  82 train_loss=36594.62827\n",
      "i=  83 train_loss=36765.24485\n",
      "i=  84 train_loss=36511.73625\n",
      "i=  85 train_loss=36872.33156\n",
      "i=  86 train_loss=36913.10828\n",
      "i=  87 train_loss=36849.95240\n",
      "i=  88 train_loss=36678.71199\n",
      "i=  89 train_loss=36545.78627\n",
      "i=  90 train_loss=36520.49314\n",
      "i=  91 train_loss=36626.27189\n",
      "i=  92 train_loss=36464.89755\n",
      "i=  93 train_loss=36756.09128\n",
      "i=  94 train_loss=36537.59372\n",
      "i=  95 train_loss=36597.00996\n",
      "i=  96 train_loss=36853.85786\n",
      "i=  97 train_loss=36641.10128\n",
      "i=  98 train_loss=36242.60512\n",
      "i=  99 train_loss=37049.42454\n",
      "i= 100 train_loss=36611.27307\n",
      "i= 101 train_loss=36676.07920\n",
      "i= 102 train_loss=36750.07582\n",
      "i= 103 train_loss=37046.54390\n",
      "i= 104 train_loss=36427.92466\n",
      "i= 105 train_loss=36617.26567\n",
      "i= 106 train_loss=36689.38174\n",
      "i= 107 train_loss=36723.28418\n",
      "i= 108 train_loss=36633.36825\n",
      "i= 109 train_loss=37319.35047\n",
      "i= 110 train_loss=36491.66190\n",
      "i= 111 train_loss=36869.20748\n",
      "i= 112 train_loss=37211.57613\n",
      "i= 113 train_loss=36240.55172\n",
      "i= 114 train_loss=36345.39833\n",
      "i= 115 train_loss=36719.42656\n",
      "i= 116 train_loss=36703.25992\n",
      "i= 117 train_loss=36637.64609\n",
      "i= 118 train_loss=36630.68694\n",
      "i= 119 train_loss=36468.73263\n",
      "i= 120 train_loss=36540.21182\n",
      "i= 121 train_loss=36643.06848\n",
      "i= 122 train_loss=36765.18068\n",
      "i= 123 train_loss=36763.39735\n",
      "i= 124 train_loss=36943.71858\n",
      "i= 125 train_loss=36404.96465\n",
      "i= 126 train_loss=36674.50541\n",
      "i= 127 train_loss=36568.33640\n",
      "i= 128 train_loss=36764.96739\n",
      "i= 129 train_loss=36714.07078\n",
      "i= 130 train_loss=36546.36522\n",
      "i= 131 train_loss=36445.79964\n",
      "i= 132 train_loss=36250.25410\n",
      "i= 133 train_loss=36675.51340\n",
      "i= 134 train_loss=36578.60773\n",
      "i= 135 train_loss=36746.36800\n",
      "i= 136 train_loss=36810.63797\n",
      "i= 137 train_loss=36923.29068\n",
      "i= 138 train_loss=36462.24113\n",
      "i= 139 train_loss=36652.46922\n",
      "i= 140 train_loss=36862.78813\n",
      "i= 141 train_loss=36534.00144\n",
      "i= 142 train_loss=36362.32948\n",
      "i= 143 train_loss=36356.55980\n",
      "i= 144 train_loss=37206.03539\n",
      "i= 145 train_loss=36652.28804\n",
      "i= 146 train_loss=36604.70769\n",
      "i= 147 train_loss=36555.68505\n",
      "i= 148 train_loss=36810.10422\n",
      "i= 149 train_loss=36696.87488\n",
      "i= 150 train_loss=37399.14047\n",
      "i= 151 train_loss=36498.98469\n",
      "i= 152 train_loss=36612.86535\n",
      "i= 153 train_loss=36745.95151\n",
      "i= 154 train_loss=36529.41310\n",
      "i= 155 train_loss=36801.52163\n",
      "i= 156 train_loss=36285.97191\n",
      "i= 157 train_loss=36831.94659\n",
      "i= 158 train_loss=36529.35615\n",
      "i= 159 train_loss=37014.30065\n",
      "i= 160 train_loss=36954.23303\n",
      "i= 161 train_loss=36904.50815\n",
      "i= 162 train_loss=36999.83670\n",
      "i= 163 train_loss=36996.64944\n",
      "i= 164 train_loss=36407.88307\n",
      "i= 165 train_loss=36742.76084\n",
      "i= 166 train_loss=36688.75172\n",
      "i= 167 train_loss=36324.90722\n",
      "i= 168 train_loss=36631.23294\n",
      "i= 169 train_loss=36727.80191\n",
      "i= 170 train_loss=36849.38418\n",
      "i= 171 train_loss=36624.60366\n",
      "i= 172 train_loss=36685.25352\n",
      "i= 173 train_loss=36606.98004\n",
      "i= 174 train_loss=36474.91618\n",
      "i= 175 train_loss=36595.76426\n",
      "i= 176 train_loss=36603.05955\n",
      "i= 177 train_loss=37320.09033\n",
      "i= 178 train_loss=36699.83797\n",
      "i= 179 train_loss=36698.61092\n",
      "i= 180 train_loss=36617.37907\n",
      "i= 181 train_loss=36619.83859\n",
      "i= 182 train_loss=36802.66841\n",
      "i= 183 train_loss=37184.46236\n",
      "i= 184 train_loss=36526.00262\n",
      "i= 185 train_loss=37290.37498\n",
      "i= 186 train_loss=36570.77918\n",
      "i= 187 train_loss=36225.34684\n",
      "i= 188 train_loss=36499.45999\n",
      "i= 189 train_loss=36959.33842\n",
      "i= 190 train_loss=36971.96968\n",
      "i= 191 train_loss=36670.74526\n",
      "i= 192 train_loss=36850.91181\n",
      "i= 193 train_loss=36716.45029\n",
      "i= 194 train_loss=36847.76977\n",
      "i= 195 train_loss=36776.13253\n",
      "i= 196 train_loss=36797.04852\n",
      "i= 197 train_loss=36677.58141\n",
      "i= 198 train_loss=36995.19251\n",
      "i= 199 train_loss=36580.98914\n",
      "i= 200 train_loss=36615.06439\n",
      "i= 201 train_loss=36719.93571\n",
      "i= 202 train_loss=36703.26884\n",
      "i= 203 train_loss=36893.12776\n",
      "i= 204 train_loss=36507.84760\n",
      "i= 205 train_loss=36520.42395\n",
      "i= 206 train_loss=36855.96536\n",
      "i= 207 train_loss=36383.01811\n",
      "i= 208 train_loss=36315.84145\n",
      "i= 209 train_loss=36624.64229\n",
      "i= 210 train_loss=36944.20617\n",
      "i= 211 train_loss=36845.34972\n",
      "i= 212 train_loss=37040.13247\n",
      "i= 213 train_loss=36869.96052\n",
      "i= 214 train_loss=36808.51598\n",
      "i= 215 train_loss=36887.62916\n",
      "i= 216 train_loss=36595.19974\n",
      "i= 217 train_loss=36601.23674\n",
      "i= 218 train_loss=36413.18702\n",
      "i= 219 train_loss=36566.48720\n",
      "i= 220 train_loss=37042.20232\n",
      "i= 221 train_loss=36768.21911\n",
      "i= 222 train_loss=36751.00706\n",
      "i= 223 train_loss=36620.86979\n",
      "i= 224 train_loss=36437.02569\n",
      "i= 225 train_loss=36680.57622\n",
      "i= 226 train_loss=36409.07666\n",
      "i= 227 train_loss=36521.45335\n",
      "i= 228 train_loss=36735.87140\n",
      "i= 229 train_loss=36568.74972\n",
      "i= 230 train_loss=36759.34828\n",
      "i= 231 train_loss=36844.84584\n",
      "i= 232 train_loss=36841.51722\n",
      "i= 233 train_loss=36630.27273\n",
      "i= 234 train_loss=36654.70138\n",
      "i= 235 train_loss=36452.66812\n",
      "i= 236 train_loss=36927.65514\n",
      "i= 237 train_loss=36673.70457\n",
      "i= 238 train_loss=37060.93813\n",
      "i= 239 train_loss=36660.53410\n",
      "i= 240 train_loss=36636.46333\n",
      "i= 241 train_loss=36824.76202\n",
      "i= 242 train_loss=36673.03568\n",
      "i= 243 train_loss=36702.65020\n",
      "i= 244 train_loss=36801.35602\n",
      "i= 245 train_loss=36727.13329\n",
      "i= 246 train_loss=36536.10616\n",
      "i= 247 train_loss=36167.14565\n",
      "i= 248 train_loss=36745.58791\n",
      "i= 249 train_loss=36331.08113\n",
      "i= 250 train_loss=36718.38384\n",
      "i= 251 train_loss=36330.09993\n",
      "i= 252 train_loss=36516.46515\n",
      "i= 253 train_loss=36466.34859\n",
      "i= 254 train_loss=37077.52269\n",
      "i= 255 train_loss=36377.11219\n",
      "i= 256 train_loss=36419.81805\n",
      "i= 257 train_loss=36324.28102\n",
      "i= 258 train_loss=36674.25878\n",
      "i= 259 train_loss=36725.61561\n",
      "i= 260 train_loss=36887.04085\n",
      "i= 261 train_loss=36309.67157\n",
      "i= 262 train_loss=36352.44813\n",
      "i= 263 train_loss=36597.53821\n",
      "i= 264 train_loss=36475.97705\n",
      "i= 265 train_loss=36308.92883\n",
      "i= 266 train_loss=36890.00523\n",
      "i= 267 train_loss=37044.34687\n",
      "i= 268 train_loss=37221.64237\n",
      "i= 269 train_loss=36709.95261\n",
      "i= 270 train_loss=36518.98913\n",
      "i= 271 train_loss=36732.61831\n",
      "i= 272 train_loss=36696.35727\n",
      "i= 273 train_loss=36176.19620\n",
      "i= 274 train_loss=36637.64876\n",
      "i= 275 train_loss=36440.90568\n",
      "i= 276 train_loss=36908.51775\n",
      "i= 277 train_loss=36940.45018\n",
      "i= 278 train_loss=37096.73697\n",
      "i= 279 train_loss=36668.38250\n",
      "i= 280 train_loss=36636.52341\n",
      "i= 281 train_loss=36884.78603\n",
      "i= 282 train_loss=36775.63657\n",
      "i= 283 train_loss=37024.98907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 284 train_loss=36555.15107\n",
      "i= 285 train_loss=36303.34228\n",
      "i= 286 train_loss=36911.85706\n",
      "i= 287 train_loss=36716.25907\n",
      "i= 288 train_loss=36440.84455\n",
      "i= 289 train_loss=36637.49784\n",
      "i= 290 train_loss=36662.65999\n",
      "i= 291 train_loss=36855.45563\n",
      "i= 292 train_loss=36596.67861\n",
      "i= 293 train_loss=36577.90707\n",
      "i= 294 train_loss=36678.15761\n",
      "i= 295 train_loss=37049.28142\n",
      "i= 296 train_loss=36492.95566\n",
      "i= 297 train_loss=36965.87196\n",
      "i= 298 train_loss=36897.38295\n",
      "i= 299 train_loss=36719.33653\n",
      "i= 300 train_loss=36543.84677\n",
      "i= 301 train_loss=36997.77905\n",
      "i= 302 train_loss=36755.57760\n",
      "i= 303 train_loss=36711.24931\n",
      "i= 304 train_loss=36767.81669\n",
      "i= 305 train_loss=36510.16423\n",
      "i= 306 train_loss=36181.48291\n",
      "i= 307 train_loss=36695.42763\n",
      "i= 308 train_loss=37106.08371\n",
      "i= 309 train_loss=36393.39164\n",
      "i= 310 train_loss=36443.70011\n",
      "i= 311 train_loss=36882.46729\n",
      "i= 312 train_loss=36508.73995\n",
      "i= 313 train_loss=36705.05728\n",
      "i= 314 train_loss=36370.25943\n",
      "i= 315 train_loss=37313.53711\n",
      "i= 316 train_loss=36639.58291\n",
      "i= 317 train_loss=36586.82752\n",
      "i= 318 train_loss=36466.74567\n",
      "i= 319 train_loss=36905.79161\n",
      "i= 320 train_loss=36458.44274\n",
      "i= 321 train_loss=37316.61308\n",
      "i= 322 train_loss=36712.67488\n",
      "i= 323 train_loss=36673.85352\n",
      "i= 324 train_loss=36870.31487\n",
      "i= 325 train_loss=36929.06122\n",
      "i= 326 train_loss=36126.05780\n",
      "i= 327 train_loss=36889.50190\n",
      "i= 328 train_loss=36622.87717\n",
      "i= 329 train_loss=36842.15919\n",
      "i= 330 train_loss=36810.28082\n",
      "i= 331 train_loss=36757.62676\n",
      "i= 332 train_loss=36464.27240\n",
      "i= 333 train_loss=36345.46712\n",
      "i= 334 train_loss=36794.77124\n",
      "i= 335 train_loss=36208.46450\n",
      "i= 336 train_loss=36336.78460\n",
      "i= 337 train_loss=36399.68030\n",
      "i= 338 train_loss=37029.61176\n",
      "i= 339 train_loss=37432.17143\n",
      "i= 340 train_loss=36465.45943\n",
      "i= 341 train_loss=36835.43545\n",
      "i= 342 train_loss=36980.11223\n",
      "i= 343 train_loss=36502.74246\n",
      "i= 344 train_loss=36440.23754\n",
      "i= 345 train_loss=36741.05146\n",
      "i= 346 train_loss=36484.70358\n",
      "i= 347 train_loss=36810.69347\n",
      "i= 348 train_loss=36632.46108\n",
      "i= 349 train_loss=36937.31814\n",
      "i= 350 train_loss=36605.04489\n",
      "i= 351 train_loss=36489.32358\n",
      "i= 352 train_loss=36780.55383\n",
      "i= 353 train_loss=36923.56716\n",
      "i= 354 train_loss=37062.28599\n",
      "i= 355 train_loss=36594.23612\n",
      "i= 356 train_loss=36862.81189\n",
      "i= 357 train_loss=36243.57257\n",
      "i= 358 train_loss=36936.58272\n",
      "i= 359 train_loss=36445.39259\n",
      "i= 360 train_loss=36759.29701\n",
      "i= 361 train_loss=36999.91997\n",
      "i= 362 train_loss=36850.75583\n",
      "i= 363 train_loss=36682.04479\n",
      "i= 364 train_loss=36421.82225\n",
      "i= 365 train_loss=36987.88221\n",
      "i= 366 train_loss=36692.28540\n",
      "i= 367 train_loss=36609.80000\n",
      "i= 368 train_loss=36931.31732\n",
      "i= 369 train_loss=36676.20526\n",
      "i= 370 train_loss=36899.12988\n",
      "i= 371 train_loss=36700.89766\n",
      "i= 372 train_loss=36404.22994\n",
      "i= 373 train_loss=36499.08960\n",
      "i= 374 train_loss=36905.75852\n",
      "i= 375 train_loss=36753.77816\n",
      "i= 376 train_loss=36918.53016\n",
      "i= 377 train_loss=36671.17392\n",
      "i= 378 train_loss=36519.08910\n",
      "i= 379 train_loss=37044.44876\n",
      "i= 380 train_loss=36667.51157\n",
      "i= 381 train_loss=36466.47954\n",
      "i= 382 train_loss=36326.44378\n",
      "i= 383 train_loss=36911.58156\n",
      "i= 384 train_loss=36422.69905\n",
      "i= 385 train_loss=36391.56582\n",
      "i= 386 train_loss=36896.37985\n",
      "i= 387 train_loss=36679.84803\n",
      "i= 388 train_loss=37142.42429\n",
      "i= 389 train_loss=36753.88763\n",
      "i= 390 train_loss=37177.91918\n",
      "i= 391 train_loss=36445.01436\n",
      "i= 392 train_loss=36613.17320\n",
      "i= 393 train_loss=36800.77353\n",
      "i= 394 train_loss=36377.30217\n",
      "i= 395 train_loss=36463.87355\n",
      "i= 396 train_loss=36394.92228\n",
      "i= 397 train_loss=37130.84240\n",
      "i= 398 train_loss=36769.08820\n",
      "i= 399 train_loss=36599.06931\n",
      "i= 400 train_loss=36441.15463\n",
      "i= 401 train_loss=36799.83875\n",
      "i= 402 train_loss=36495.88613\n",
      "i= 403 train_loss=36729.23485\n",
      "i= 404 train_loss=36570.30588\n",
      "i= 405 train_loss=36928.09510\n",
      "i= 406 train_loss=36353.48630\n",
      "i= 407 train_loss=36835.75114\n",
      "i= 408 train_loss=36394.85620\n",
      "i= 409 train_loss=36750.48875\n",
      "i= 410 train_loss=36515.84519\n",
      "i= 411 train_loss=36586.70482\n",
      "i= 412 train_loss=37379.89178\n",
      "i= 413 train_loss=36481.61338\n",
      "i= 414 train_loss=36583.45715\n",
      "i= 415 train_loss=36525.01449\n",
      "i= 416 train_loss=37014.69413\n",
      "i= 417 train_loss=36321.58995\n",
      "i= 418 train_loss=36666.07764\n",
      "i= 419 train_loss=36564.26325\n",
      "i= 420 train_loss=36310.61276\n",
      "i= 421 train_loss=36753.10232\n",
      "i= 422 train_loss=36925.69139\n",
      "i= 423 train_loss=36869.15096\n",
      "i= 424 train_loss=36606.88386\n",
      "i= 425 train_loss=36614.33847\n",
      "i= 426 train_loss=36551.11987\n",
      "i= 427 train_loss=36576.88802\n",
      "i= 428 train_loss=36771.14872\n",
      "i= 429 train_loss=36645.09967\n",
      "i= 430 train_loss=37137.37941\n",
      "i= 431 train_loss=36129.21947\n",
      "i= 432 train_loss=36992.18085\n",
      "i= 433 train_loss=36843.37738\n",
      "i= 434 train_loss=36717.18049\n",
      "i= 435 train_loss=36561.39374\n",
      "i= 436 train_loss=36831.32401\n",
      "i= 437 train_loss=36522.94344\n",
      "i= 438 train_loss=36823.34186\n",
      "i= 439 train_loss=36552.81111\n",
      "i= 440 train_loss=36436.96711\n",
      "i= 441 train_loss=36638.65986\n",
      "i= 442 train_loss=36611.61457\n",
      "i= 443 train_loss=36546.72604\n",
      "i= 444 train_loss=36632.17428\n",
      "i= 445 train_loss=36787.73195\n",
      "i= 446 train_loss=36361.95788\n",
      "i= 447 train_loss=36929.31539\n",
      "i= 448 train_loss=36802.68300\n",
      "i= 449 train_loss=37034.20638\n",
      "i= 450 train_loss=36478.75511\n",
      "i= 451 train_loss=36797.79555\n",
      "i= 452 train_loss=36543.17901\n",
      "i= 453 train_loss=36795.19802\n",
      "i= 454 train_loss=36622.66224\n",
      "i= 455 train_loss=37130.15015\n",
      "i= 456 train_loss=36544.14919\n",
      "i= 457 train_loss=36600.05403\n",
      "i= 458 train_loss=36795.17542\n",
      "i= 459 train_loss=36454.95822\n",
      "i= 460 train_loss=36636.41794\n",
      "i= 461 train_loss=36864.49963\n",
      "i= 462 train_loss=36871.34291\n",
      "i= 463 train_loss=36662.41012\n",
      "i= 464 train_loss=36444.79339\n",
      "i= 465 train_loss=36374.15939\n",
      "i= 466 train_loss=36418.50436\n",
      "i= 467 train_loss=37036.87846\n",
      "i= 468 train_loss=36412.14081\n",
      "i= 469 train_loss=36632.59900\n",
      "i= 470 train_loss=36480.05126\n",
      "i= 471 train_loss=36714.74312\n",
      "i= 472 train_loss=36643.27042\n",
      "i= 473 train_loss=37016.15526\n",
      "i= 474 train_loss=36549.16737\n",
      "i= 475 train_loss=36771.28212\n",
      "i= 476 train_loss=36502.07522\n",
      "i= 477 train_loss=36532.04676\n",
      "i= 478 train_loss=36605.59661\n",
      "i= 479 train_loss=36694.84324\n",
      "i= 480 train_loss=37024.24994\n",
      "i= 481 train_loss=36992.81830\n",
      "i= 482 train_loss=36407.52549\n",
      "i= 483 train_loss=36390.87732\n",
      "i= 484 train_loss=36875.33615\n",
      "i= 485 train_loss=37219.02380\n",
      "i= 486 train_loss=36835.62951\n",
      "i= 487 train_loss=36684.21754\n",
      "i= 488 train_loss=36497.01605\n",
      "i= 489 train_loss=36988.92265\n",
      "i= 490 train_loss=36459.13815\n",
      "i= 491 train_loss=36383.32058\n",
      "i= 492 train_loss=36700.96584\n",
      "i= 493 train_loss=36880.52148\n",
      "i= 494 train_loss=36497.21744\n",
      "i= 495 train_loss=36728.65094\n",
      "i= 496 train_loss=36815.15379\n",
      "i= 497 train_loss=36463.64059\n",
      "i= 498 train_loss=37020.22575\n",
      "i= 499 train_loss=36924.64452\n",
      "i= 500 train_loss=36640.88862\n",
      "i= 501 train_loss=36380.95408\n",
      "i= 502 train_loss=36837.39983\n",
      "i= 503 train_loss=36562.07648\n",
      "i= 504 train_loss=36530.92579\n",
      "i= 505 train_loss=36294.11961\n",
      "i= 506 train_loss=36548.36150\n",
      "i= 507 train_loss=36466.74302\n",
      "i= 508 train_loss=36792.32611\n",
      "i= 509 train_loss=36487.18683\n",
      "i= 510 train_loss=36578.02239\n",
      "i= 511 train_loss=36657.45422\n",
      "i= 512 train_loss=36698.22208\n",
      "i= 513 train_loss=36416.18949\n",
      "i= 514 train_loss=37132.92556\n",
      "i= 515 train_loss=36638.40846\n",
      "i= 516 train_loss=36451.46939\n",
      "i= 517 train_loss=36418.66215\n",
      "i= 518 train_loss=36888.76837\n",
      "i= 519 train_loss=36561.15771\n",
      "i= 520 train_loss=36874.58315\n",
      "i= 521 train_loss=36582.83023\n",
      "i= 522 train_loss=36359.87731\n",
      "i= 523 train_loss=36450.04210\n",
      "i= 524 train_loss=36436.45789\n",
      "i= 525 train_loss=37066.99007\n",
      "i= 526 train_loss=36344.45136\n",
      "i= 527 train_loss=36847.44289\n",
      "i= 528 train_loss=36468.33332\n",
      "i= 529 train_loss=36394.32235\n",
      "i= 530 train_loss=36759.79066\n",
      "i= 531 train_loss=36794.03911\n",
      "i= 532 train_loss=36979.02153\n",
      "i= 533 train_loss=36429.15276\n",
      "i= 534 train_loss=37194.15060\n",
      "i= 535 train_loss=37079.37353\n",
      "i= 536 train_loss=36536.63041\n",
      "i= 537 train_loss=36470.12014\n",
      "i= 538 train_loss=36720.59585\n",
      "i= 539 train_loss=36471.94002\n",
      "i= 540 train_loss=36441.07223\n",
      "i= 541 train_loss=36579.73651\n",
      "i= 542 train_loss=36641.57553\n",
      "i= 543 train_loss=36754.65336\n",
      "i= 544 train_loss=36382.94397\n",
      "i= 545 train_loss=36764.55337\n",
      "i= 546 train_loss=36577.68791\n",
      "i= 547 train_loss=36203.37465\n",
      "i= 548 train_loss=36615.08623\n",
      "i= 549 train_loss=36250.82564\n",
      "i= 550 train_loss=36543.39177\n",
      "i= 551 train_loss=36802.57124\n",
      "i= 552 train_loss=36255.10501\n",
      "i= 553 train_loss=36882.25872\n",
      "i= 554 train_loss=36763.40057\n",
      "i= 555 train_loss=36806.92405\n",
      "i= 556 train_loss=36240.48245\n",
      "i= 557 train_loss=36233.58817\n",
      "i= 558 train_loss=36332.55079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 559 train_loss=36865.74223\n",
      "i= 560 train_loss=36653.82981\n",
      "i= 561 train_loss=37069.74626\n",
      "i= 562 train_loss=36691.43556\n",
      "i= 563 train_loss=36875.97970\n",
      "i= 564 train_loss=36467.56584\n",
      "i= 565 train_loss=36652.78637\n",
      "i= 566 train_loss=36429.60437\n",
      "i= 567 train_loss=36436.51283\n",
      "i= 568 train_loss=36433.28027\n",
      "i= 569 train_loss=36545.86109\n",
      "i= 570 train_loss=36934.28909\n",
      "i= 571 train_loss=36375.82233\n",
      "i= 572 train_loss=36711.96956\n",
      "i= 573 train_loss=36790.46247\n",
      "i= 574 train_loss=36303.17018\n",
      "i= 575 train_loss=36778.39680\n",
      "i= 576 train_loss=36418.71910\n",
      "i= 577 train_loss=36877.76058\n",
      "i= 578 train_loss=36597.44919\n",
      "i= 579 train_loss=36798.64058\n",
      "i= 580 train_loss=36557.27853\n",
      "i= 581 train_loss=36626.73300\n",
      "i= 582 train_loss=36735.31168\n",
      "i= 583 train_loss=36840.05640\n",
      "i= 584 train_loss=36777.73969\n",
      "i= 585 train_loss=36829.98103\n",
      "i= 586 train_loss=36828.63800\n",
      "i= 587 train_loss=36855.49605\n",
      "i= 588 train_loss=36639.87229\n",
      "i= 589 train_loss=36898.14876\n",
      "i= 590 train_loss=36603.54706\n",
      "i= 591 train_loss=37217.43728\n",
      "i= 592 train_loss=36781.98846\n",
      "i= 593 train_loss=36710.99822\n",
      "i= 594 train_loss=36588.45255\n",
      "i= 595 train_loss=36515.19250\n",
      "i= 596 train_loss=36233.86978\n",
      "i= 597 train_loss=36520.31568\n",
      "i= 598 train_loss=36330.46797\n",
      "i= 599 train_loss=36587.35442\n",
      "i= 600 train_loss=36410.33792\n",
      "i= 601 train_loss=36758.03662\n",
      "i= 602 train_loss=36803.03126\n",
      "i= 603 train_loss=36758.28549\n",
      "i= 604 train_loss=36206.26842\n",
      "i= 605 train_loss=37100.79765\n",
      "i= 606 train_loss=36490.44817\n",
      "i= 607 train_loss=36467.08581\n",
      "i= 608 train_loss=36648.83218\n",
      "i= 609 train_loss=36732.32677\n",
      "i= 610 train_loss=36729.94618\n",
      "i= 611 train_loss=36545.70673\n",
      "i= 612 train_loss=36767.89725\n",
      "i= 613 train_loss=37119.37508\n",
      "i= 614 train_loss=36432.86115\n",
      "i= 615 train_loss=36979.51784\n",
      "i= 616 train_loss=36699.70031\n",
      "i= 617 train_loss=36327.11818\n",
      "i= 618 train_loss=36476.74252\n",
      "i= 619 train_loss=36887.91900\n",
      "i= 620 train_loss=36467.56728\n",
      "i= 621 train_loss=36265.87171\n",
      "i= 622 train_loss=36931.90081\n",
      "i= 623 train_loss=36647.61506\n",
      "i= 624 train_loss=36660.47540\n",
      "i= 625 train_loss=36564.23954\n",
      "i= 626 train_loss=36810.07361\n",
      "i= 627 train_loss=36588.30511\n",
      "i= 628 train_loss=36647.78649\n",
      "i= 629 train_loss=36675.75812\n",
      "i= 630 train_loss=36306.93192\n",
      "i= 631 train_loss=36395.30170\n",
      "i= 632 train_loss=36558.71244\n",
      "i= 633 train_loss=36964.03109\n",
      "i= 634 train_loss=36566.79023\n",
      "i= 635 train_loss=36597.74025\n",
      "i= 636 train_loss=36808.19441\n",
      "i= 637 train_loss=36617.62898\n",
      "i= 638 train_loss=36397.59290\n",
      "i= 639 train_loss=36513.55325\n",
      "i= 640 train_loss=36678.85524\n",
      "i= 641 train_loss=36594.41597\n",
      "i= 642 train_loss=36468.66440\n",
      "i= 643 train_loss=36455.71767\n",
      "i= 644 train_loss=36565.35767\n",
      "i= 645 train_loss=36809.84924\n",
      "i= 646 train_loss=36003.68122\n",
      "i= 647 train_loss=36852.97880\n",
      "i= 648 train_loss=36383.77205\n",
      "i= 649 train_loss=37021.62490\n",
      "i= 650 train_loss=36587.24612\n",
      "i= 651 train_loss=36581.05226\n",
      "i= 652 train_loss=36985.28978\n",
      "i= 653 train_loss=36735.45353\n",
      "i= 654 train_loss=36459.91157\n",
      "i= 655 train_loss=36645.64184\n",
      "i= 656 train_loss=37056.26823\n",
      "i= 657 train_loss=36725.44747\n",
      "i= 658 train_loss=37095.59624\n",
      "i= 659 train_loss=36423.34794\n",
      "i= 660 train_loss=36865.58698\n",
      "i= 661 train_loss=36451.19835\n",
      "i= 662 train_loss=36839.20559\n",
      "i= 663 train_loss=36529.59228\n",
      "i= 664 train_loss=36331.56564\n",
      "i= 665 train_loss=36346.65383\n",
      "i= 666 train_loss=36448.38956\n",
      "i= 667 train_loss=36166.54219\n",
      "i= 668 train_loss=36519.77608\n",
      "i= 669 train_loss=36530.23436\n",
      "i= 670 train_loss=36671.05481\n",
      "i= 671 train_loss=36615.67602\n",
      "i= 672 train_loss=36889.75193\n",
      "i= 673 train_loss=36862.48705\n",
      "i= 674 train_loss=36557.98108\n",
      "i= 675 train_loss=36573.70752\n",
      "i= 676 train_loss=36625.41808\n",
      "i= 677 train_loss=37069.31019\n",
      "i= 678 train_loss=36218.46823\n",
      "i= 679 train_loss=36753.00378\n",
      "i= 680 train_loss=36997.46851\n",
      "i= 681 train_loss=36463.86710\n",
      "i= 682 train_loss=36573.98317\n",
      "i= 683 train_loss=36341.24765\n",
      "i= 684 train_loss=36993.48892\n",
      "i= 685 train_loss=36567.27768\n",
      "i= 686 train_loss=36724.05758\n",
      "i= 687 train_loss=36871.60348\n",
      "i= 688 train_loss=36896.38453\n",
      "i= 689 train_loss=36888.44974\n",
      "i= 690 train_loss=36363.18309\n",
      "i= 691 train_loss=36346.22314\n",
      "i= 692 train_loss=36530.84959\n",
      "i= 693 train_loss=36544.29234\n",
      "i= 694 train_loss=36553.42898\n",
      "i= 695 train_loss=36421.99495\n",
      "i= 696 train_loss=36478.04396\n",
      "i= 697 train_loss=36397.04044\n",
      "i= 698 train_loss=36609.10524\n",
      "i= 699 train_loss=36967.92774\n",
      "i= 700 train_loss=36535.73906\n",
      "i= 701 train_loss=37080.77379\n",
      "i= 702 train_loss=36585.83732\n",
      "i= 703 train_loss=36849.09513\n",
      "i= 704 train_loss=36477.69745\n",
      "i= 705 train_loss=36516.46027\n",
      "i= 706 train_loss=36829.46636\n",
      "i= 707 train_loss=36604.60029\n",
      "i= 708 train_loss=36115.37114\n",
      "i= 709 train_loss=36343.62830\n",
      "i= 710 train_loss=36953.45425\n",
      "i= 711 train_loss=36582.12051\n",
      "i= 712 train_loss=36610.02754\n",
      "i= 713 train_loss=36644.69996\n",
      "i= 714 train_loss=36946.60621\n",
      "i= 715 train_loss=36403.37077\n",
      "i= 716 train_loss=36692.82883\n",
      "i= 717 train_loss=36769.72243\n",
      "i= 718 train_loss=36696.03133\n",
      "i= 719 train_loss=36507.48712\n",
      "i= 720 train_loss=36807.16913\n",
      "i= 721 train_loss=37038.14388\n",
      "i= 722 train_loss=36542.82547\n",
      "i= 723 train_loss=36733.14817\n",
      "i= 724 train_loss=36591.97678\n",
      "i= 725 train_loss=36779.15604\n",
      "i= 726 train_loss=36134.56007\n",
      "i= 727 train_loss=36572.31850\n",
      "i= 728 train_loss=36711.87869\n",
      "i= 729 train_loss=36504.42995\n",
      "i= 730 train_loss=36635.23864\n",
      "i= 731 train_loss=37073.79119\n",
      "i= 732 train_loss=36664.45124\n",
      "i= 733 train_loss=36707.08081\n",
      "i= 734 train_loss=36791.59072\n",
      "i= 735 train_loss=36731.02313\n",
      "i= 736 train_loss=36374.28803\n",
      "i= 737 train_loss=36809.92775\n",
      "i= 738 train_loss=36317.66052\n",
      "i= 739 train_loss=36974.18640\n",
      "i= 740 train_loss=37028.75237\n",
      "i= 741 train_loss=36955.62792\n",
      "i= 742 train_loss=36991.54206\n",
      "i= 743 train_loss=36376.89976\n",
      "i= 744 train_loss=36938.66932\n",
      "i= 745 train_loss=36435.45758\n",
      "i= 746 train_loss=36224.81351\n",
      "i= 747 train_loss=36529.66256\n",
      "i= 748 train_loss=36519.17878\n",
      "i= 749 train_loss=36512.47265\n",
      "i= 750 train_loss=36867.04033\n",
      "i= 751 train_loss=36514.78096\n",
      "i= 752 train_loss=36632.76333\n",
      "i= 753 train_loss=36896.42984\n",
      "i= 754 train_loss=36959.31207\n",
      "i= 755 train_loss=36375.98264\n",
      "i= 756 train_loss=37160.49840\n",
      "i= 757 train_loss=36399.79995\n",
      "i= 758 train_loss=36711.23207\n",
      "i= 759 train_loss=36323.07797\n",
      "i= 760 train_loss=36736.12111\n",
      "i= 761 train_loss=36439.29221\n",
      "i= 762 train_loss=36758.97504\n",
      "i= 763 train_loss=36556.83683\n",
      "i= 764 train_loss=37000.09152\n",
      "i= 765 train_loss=36340.80072\n",
      "i= 766 train_loss=36737.21436\n",
      "i= 767 train_loss=36446.76963\n",
      "i= 768 train_loss=36681.77813\n",
      "i= 769 train_loss=36824.40637\n",
      "i= 770 train_loss=36305.02620\n",
      "i= 771 train_loss=36843.63173\n",
      "i= 772 train_loss=36835.98373\n",
      "i= 773 train_loss=36577.43132\n",
      "i= 774 train_loss=36244.18710\n",
      "i= 775 train_loss=36227.02167\n",
      "i= 776 train_loss=37028.65680\n",
      "i= 777 train_loss=37088.46430\n",
      "i= 778 train_loss=36434.79263\n",
      "i= 779 train_loss=36617.32803\n",
      "i= 780 train_loss=36691.94895\n",
      "i= 781 train_loss=36808.39390\n",
      "i= 782 train_loss=36621.80265\n",
      "i= 783 train_loss=36467.09449\n",
      "i= 784 train_loss=36650.38052\n",
      "i= 785 train_loss=36539.40577\n",
      "i= 786 train_loss=36238.22545\n",
      "i= 787 train_loss=36590.27830\n",
      "i= 788 train_loss=36382.06243\n",
      "i= 789 train_loss=36347.19808\n",
      "i= 790 train_loss=36752.59163\n",
      "i= 791 train_loss=36435.64164\n",
      "i= 792 train_loss=36645.59573\n",
      "i= 793 train_loss=36500.83967\n",
      "i= 794 train_loss=36804.57992\n",
      "i= 795 train_loss=36417.96674\n",
      "i= 796 train_loss=36956.08745\n",
      "i= 797 train_loss=36857.64698\n",
      "i= 798 train_loss=36987.51630\n",
      "i= 799 train_loss=36528.36896\n",
      "i= 800 train_loss=36550.97945\n",
      "i= 801 train_loss=37073.22986\n",
      "i= 802 train_loss=36513.07101\n",
      "i= 803 train_loss=36613.25417\n",
      "i= 804 train_loss=37089.26819\n",
      "i= 805 train_loss=36857.22925\n",
      "i= 806 train_loss=36754.81924\n",
      "i= 807 train_loss=36578.80300\n",
      "i= 808 train_loss=37013.31116\n",
      "i= 809 train_loss=36633.05776\n",
      "i= 810 train_loss=36812.82823\n",
      "i= 811 train_loss=36173.94925\n",
      "i= 812 train_loss=36831.18246\n",
      "i= 813 train_loss=37206.99820\n",
      "i= 814 train_loss=36792.71258\n",
      "i= 815 train_loss=36809.41410\n",
      "i= 816 train_loss=36825.16943\n",
      "i= 817 train_loss=36475.72940\n",
      "i= 818 train_loss=36472.98825\n",
      "i= 819 train_loss=36735.85979\n",
      "i= 820 train_loss=36371.22253\n",
      "i= 821 train_loss=36712.36202\n",
      "i= 822 train_loss=36254.65228\n",
      "i= 823 train_loss=36466.55653\n",
      "i= 824 train_loss=36831.02962\n",
      "i= 825 train_loss=36648.08583\n",
      "i= 826 train_loss=36735.11071\n",
      "i= 827 train_loss=36423.53575\n",
      "i= 828 train_loss=36933.33431\n",
      "i= 829 train_loss=36371.66207\n",
      "i= 830 train_loss=36751.69897\n",
      "i= 831 train_loss=36596.07118\n",
      "i= 832 train_loss=36488.97657\n",
      "i= 833 train_loss=36566.42925\n",
      "i= 834 train_loss=36961.70171\n",
      "i= 835 train_loss=36280.64913\n",
      "i= 836 train_loss=36635.63103\n",
      "i= 837 train_loss=36403.56660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 838 train_loss=36258.37510\n",
      "i= 839 train_loss=36549.04819\n",
      "i= 840 train_loss=36279.35808\n",
      "i= 841 train_loss=36772.55661\n",
      "i= 842 train_loss=36564.67439\n",
      "i= 843 train_loss=36139.56826\n",
      "i= 844 train_loss=36396.59578\n",
      "i= 845 train_loss=36517.65764\n",
      "i= 846 train_loss=36456.82133\n",
      "i= 847 train_loss=36187.07576\n",
      "i= 848 train_loss=36366.05675\n",
      "i= 849 train_loss=36435.59928\n",
      "i= 850 train_loss=36828.45105\n",
      "i= 851 train_loss=36525.54406\n",
      "i= 852 train_loss=36719.48694\n",
      "i= 853 train_loss=36846.45812\n",
      "i= 854 train_loss=36339.33067\n",
      "i= 855 train_loss=36723.58250\n",
      "i= 856 train_loss=36366.63086\n",
      "i= 857 train_loss=36654.23096\n",
      "i= 858 train_loss=36865.87133\n",
      "i= 859 train_loss=37127.69320\n",
      "i= 860 train_loss=37181.45864\n",
      "i= 861 train_loss=36884.70075\n",
      "i= 862 train_loss=36820.46922\n",
      "i= 863 train_loss=36443.40136\n",
      "i= 864 train_loss=36831.60353\n",
      "i= 865 train_loss=36518.12902\n",
      "i= 866 train_loss=36993.78160\n",
      "i= 867 train_loss=36588.95298\n",
      "i= 868 train_loss=36790.91975\n",
      "i= 869 train_loss=36286.53740\n",
      "i= 870 train_loss=36226.49070\n",
      "i= 871 train_loss=36681.69774\n",
      "i= 872 train_loss=36742.66214\n",
      "i= 873 train_loss=36782.70382\n",
      "i= 874 train_loss=36410.96894\n",
      "i= 875 train_loss=37060.96517\n",
      "i= 876 train_loss=36626.26759\n",
      "i= 877 train_loss=36380.06984\n",
      "i= 878 train_loss=36195.61809\n",
      "i= 879 train_loss=36596.65183\n",
      "i= 880 train_loss=37013.50819\n",
      "i= 881 train_loss=36444.02988\n",
      "i= 882 train_loss=36540.77151\n",
      "i= 883 train_loss=36534.44191\n",
      "i= 884 train_loss=36643.30270\n",
      "i= 885 train_loss=36768.56452\n",
      "i= 886 train_loss=36325.37431\n",
      "i= 887 train_loss=36660.78338\n",
      "i= 888 train_loss=36500.47477\n",
      "i= 889 train_loss=36476.46229\n",
      "i= 890 train_loss=36586.62294\n",
      "i= 891 train_loss=36599.02073\n",
      "i= 892 train_loss=36561.80276\n",
      "i= 893 train_loss=36580.93208\n",
      "i= 894 train_loss=37053.20552\n",
      "i= 895 train_loss=36713.31527\n",
      "i= 896 train_loss=36709.58865\n",
      "i= 897 train_loss=37203.43768\n",
      "i= 898 train_loss=36525.33508\n",
      "i= 899 train_loss=36319.64244\n",
      "i= 900 train_loss=36867.86171\n",
      "i= 901 train_loss=36638.42638\n",
      "i= 902 train_loss=36771.43209\n",
      "i= 903 train_loss=36530.61976\n",
      "i= 904 train_loss=37380.98867\n",
      "i= 905 train_loss=36393.82161\n",
      "i= 906 train_loss=36502.93320\n",
      "i= 907 train_loss=36491.25953\n",
      "i= 908 train_loss=36327.59966\n",
      "i= 909 train_loss=37056.66411\n",
      "i= 910 train_loss=36669.98629\n",
      "i= 911 train_loss=36557.83935\n",
      "i= 912 train_loss=36423.04407\n",
      "i= 913 train_loss=37029.82259\n",
      "i= 914 train_loss=36439.22057\n",
      "i= 915 train_loss=36973.76967\n",
      "i= 916 train_loss=36670.22715\n",
      "i= 917 train_loss=36483.86964\n",
      "i= 918 train_loss=36835.82037\n",
      "i= 919 train_loss=36759.11690\n",
      "i= 920 train_loss=36712.35305\n",
      "i= 921 train_loss=37019.98734\n",
      "i= 922 train_loss=36435.67543\n",
      "i= 923 train_loss=36576.80019\n",
      "i= 924 train_loss=36761.11945\n",
      "i= 925 train_loss=36229.69314\n",
      "i= 926 train_loss=36952.59684\n",
      "i= 927 train_loss=37042.15728\n",
      "i= 928 train_loss=36812.79762\n",
      "i= 929 train_loss=36566.00355\n",
      "i= 930 train_loss=36615.33667\n",
      "i= 931 train_loss=36488.64762\n",
      "i= 932 train_loss=36663.53451\n",
      "i= 933 train_loss=36381.38897\n",
      "i= 934 train_loss=36887.81293\n",
      "i= 935 train_loss=36515.40189\n",
      "i= 936 train_loss=36576.92957\n",
      "i= 937 train_loss=18365.05436\n",
      "[epoch 000] average loss: 572.8639 --New Record--\n",
      "[epoch 001] average loss: 572.4418 --New Record--\n",
      "[epoch 002] average loss: 571.9835 --New Record--\n",
      "[epoch 003] average loss: 571.4679 --New Record--\n",
      "[epoch 004] average loss: 570.9798 --New Record--\n",
      "[epoch 005] average loss: 570.6447 --New Record--\n",
      "[epoch 006] average loss: 570.0604 --New Record--\n",
      "[epoch 007] average loss: 569.4096 --New Record--\n",
      "[epoch 008] average loss: 568.9979 --New Record--\n",
      "[epoch 009] average loss: 568.2041 --New Record--\n",
      "[epoch 010] average loss: 567.4008 --New Record--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b09167188c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-87b4bc56505e>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(svi, dataloader, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 52\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0magainst\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mguide_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     model_trace = poutine.trace(poutine.replay(model, trace=guide_trace),\n\u001b[1;32m     44\u001b[0m                                 graph_type=graph_type).get_trace(*args, **kwargs)\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b1f910326ecc>\u001b[0m in \u001b[0;36mguide\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mweights_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcentrations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0massignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'assignment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/poutine/plate_messenger.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPlateMessenger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorized\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/pyro/lib/python3.6/site-packages/pyro/poutine/indep_messenger.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DIM_ALLOCATOR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIndepMessenger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_history = list()\n",
    "min_loss = 999999\n",
    "\n",
    "### Save the parameters beofre starting\n",
    "write_dir  = '/Users/ldalessi/ENUMERATION_MNIST/ARCHIVE/' \n",
    "name_vae  = \"vae_v5_\"\n",
    "name_loss = \"loss_history_v5_\"\n",
    "save_obj(params,write_dir,\"params_model_v5\")\n",
    "dataloader = trainloader\n",
    "\n",
    "\n",
    "# training loop\n",
    "for epoch in range(0,NUM_EPOCHS):\n",
    "    vae.train()    \n",
    "    loss = train_one_epoch(svi, dataloader, verbose=(epoch==0))\n",
    "    loss_history.append(loss)\n",
    "    if(loss_history[-1] < min_loss):\n",
    "        print(\"[epoch %03d] average loss: %.4f --New Record--\" % (epoch, loss)) \n",
    "        min_loss = loss_history[-1]\n",
    "        # Save if you got a record \n",
    "        save_model(vae,write_dir,name_vae+str(epoch))       \n",
    "        save_obj(loss_history,write_dir,name_loss+str(epoch))\n",
    "   \n",
    "    elif((epoch % 20) == 0 or epoch < 4):\n",
    "        \n",
    "        print(\"[epoch %03d] average loss: %.4f\" % (epoch, loss))\n",
    "        # Save at the beginning and every 20 epochs\n",
    "        save_model(vae,write_dir,name_vae+str(epoch))       \n",
    "        save_obj(loss_history,write_dir,name_loss+str(epoch))\n",
    "        \n",
    "    else:\n",
    "        print(\"[epoch %03d] average loss: %.4f\" % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,labels = next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyro]",
   "language": "python",
   "name": "conda-env-pyro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
